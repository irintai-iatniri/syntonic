"""
Resonant Linear Layer - Projection layer backed by Resonant Engine.

This module implements ResonantLinear, a pure Rust-backed linear layer
using ResonantTensor for weights and bias. All parameters natively
inhabit the golden field Q(φ).

.
"""

from __future__ import annotations

import math
from typing import Optional

import syntonic.sn as sn
from syntonic.nn.resonant_tensor import ResonantTensor

PHI = (1 + math.sqrt(5)) / 2


class ResonantLinear(sn.Module):
    """
    Linear transformation layer using Resonant Engine.

    Y = XW^T + b

    Weights and biases are stored as ResonantTensors, meaning they
    natively inhabit the Q(φ) lattice with exact arithmetic.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        precision: int = 100,
        device: str = "cpu",
        resonance_target: Optional[str] = None,
        mode: Optional[str] = None,
    ):
        """
        Initialize resonant linear layer.

        Args:
            in_features: Size of each input sample
            out_features: Size of each output sample
            bias: If set to False, the layer will not learn an additive bias
            precision: Bit precision for exact lattice arithmetic
            device: Device to place layer on
            resonance_target: Optional physics resonance target
            mode: Optional DHSR mode ('differentiation', 'harmonization', 'vacuum')
                  Affects initialization scaling based on SRT principles.
        """
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.precision = precision
        self.bias_enabled = bias
        self._device = device
        self.resonance_target = resonance_target
        self.mode = mode

        # 1. Initialize weight parameter
        # We use 'golden' initialization which respects the lattice structure
        self.weight = sn.Parameter(
            shape=[out_features, in_features],
            init="golden",
            requires_grad=True,
            precision=precision,
            device=device,
        )

        # 2. Apply mode-based scaling (DHSR modes)
        if self.mode:
            self._apply_mode_scaling()

        # 3. Apply Part IV Physics (If requested)
        if self.resonance_target:
            self._apply_physics_priors()

        # 4. Initialize bias parameter
        if bias:
            self.bias = sn.Parameter(
                shape=[out_features],
                init="golden",
                requires_grad=True,
                precision=precision,
                device=device,
            )
        else:
            self.register_buffer("bias", None)

    def _apply_mode_scaling(self):
        """
        Apply DHSR mode-based scaling to weights.

        Modes affect initialization based on SRT principles:
        - 'differentiation': Higher variance (phi scaling) to expand features
        - 'harmonization': Lower variance (1/phi scaling) to compress/condense
        - 'vacuum': Identity-like with minimal perturbation
        """
        weight_data = self.weight.tensor.to_floats()

        if self.mode == "differentiation":
            # Expand: scale by phi for differentiation
            scale = PHI
        elif self.mode == "harmonization":
            # Compress: scale by 1/phi for harmonization
            scale = 1.0 / PHI
        elif self.mode == "vacuum":
            # Vacuum: near-identity with small perturbation
            # Scale down significantly to act as subtle substrate
            scale = 0.1
        else:
            # Unknown mode, no scaling
            return

        scaled_data = [x * scale for x in weight_data]
        self.weight.tensor = ResonantTensor(
            scaled_data, list(self.weight.tensor.shape), precision=self.precision
        )

    def _apply_physics_priors(self):
        """
        Overwrites random weights with exact SRT predictions.
        This forces the layer to start at a 'Solved' state.
        """
        target_value = 1.0

        if self.resonance_target == "top_quark":
            # Formula: v * e^(2*phi) * corrections
            # We use 1.0 as base unit, scaling handled by network depth usually,
            # but here we encode the relative ratio.
            base_mass = 172.72
            target_value = base_mass

        elif self.resonance_target == "higgs":
            # Formula: 125.25 GeV
            target_value = 125.25

        elif self.resonance_target == "electron_gap":
            # The 1000 nm target from Photonic Crystal
            target_value = 1000.0

        # Apply to tensor
        # We set the diagonal or primary component to this value
        # This gives the matrix the correct 'Eigenvalue' (Energy Level)
        print(
            f"[*] Snapping ResonantLinear to {self.resonance_target} ({target_value})"
        )

        # Note: We need a method on ResonantTensor to fill or set values.
        # Assuming .fill_diagonal_() or similar exists in Rust backend,
        # or we re-create the tensor data.
        # For now, we simulate by re-initializing data:
        current_data = self.weight.tensor.to_list()  # Hypothetical accessor

        # Set the 'Resonant Frequency' of the matrix
        # In a linear layer, the spectral radius (largest eigenvalue) determines the scaling.
        # We normalize the matrix so its spectral radius matches the target.
        self.weight.tensor.normalize_spectral_radius_(target_value)

    def forward(self, x: ResonantTensor) -> ResonantTensor:
        """
        Forward pass using native Q(φ) matrix multiplication.

        Args:
            x: Input tensor of shape [..., in_features]

        Returns:
            Output tensor of shape [..., out_features]
        """
        # Handle arbitrary input dimensions by flattening
        original_shape = x.shape
        if len(original_shape) > 2:
            # Flatten: [batch, seq, features] -> [batch*seq, features]
            # Use -1 to infer batch*seq size
            x_flat = x.view([-1, original_shape[-1]])

            # Y = X_flat @ W^T using matmul method
            out = x_flat.matmul(self.weight.tensor)

            # Add bias
            if self.bias is not None:
                out.add_bias(self.bias.tensor)

            # Restoring shape: [batch*seq, out_features] -> [batch, seq, out_features]
            new_shape = list(original_shape[:-1]) + [self.out_features]
            return out.view(new_shape)
        else:
            # Standard 2D case using matmul method
            out = x.matmul(self.weight.tensor)
            if self.bias is not None:
                out.add_bias(self.bias.tensor)
            return out

    def extra_repr(self) -> str:
        return f"in_features={self.in_features}, out_features={self.out_features}, bias={self.bias_enabled}"


if __name__ == "__main__":
    # Test the pure ResonantLinear
    print("Testing ResonantLinear...")

    layer = ResonantLinear(4, 8, bias=True)
    print(f"Layer: {layer}")
    print(f"Weight parameter: {layer.weight}")
    print(f"Weight tensor shape: {layer.weight.tensor.shape}")

    # Create input
    x_data = [0.5, 0.3, -0.2, 0.8] * 2  # batch of 2
    x = ResonantTensor(x_data, [2, 4])

    # Forward pass
    y = layer.forward(x)
    print(f"Output shape: {y.shape}")
    print(f"Output syntony: {y.syntony:.4f}")
    print("SUCCESS")
