"""
Syntonic Network (sn) - Pure Python neural network base classes.

Replaces torch.nn with pure Python/ResonantTensor-based implementations.

This module provides:
- Module: Base class for all syntonic network components
- Parameter: Learnable parameter wrapper
- Sequential: Container for sequential layers
- ModuleList: Container for a list of modules
"""

from __future__ import annotations
from typing import List, Dict, Iterator, Optional, Any, Callable
import math
import random

from syntonic.nn.resonant_tensor import ResonantTensor

PHI = (1 + math.sqrt(5)) / 2


class Parameter:
    """
    Learnable parameter for syntonic networks.

    Wraps a ResonantTensor as a trainable parameter.

    Mode Norm Theory:
    -----------------
    Parameters use 1D flattened sequential mode norms: mode_norm[i] = i².
    This reflects the recursion hierarchy where earlier parameters (low i)
    are more fundamental than later parameters (high i).

    For a weight matrix [out_features, in_features]:
      - W[0,0] → index 0 → mode_norm = 0 (most fundamental)
      - W[0,1] → index 1 → mode_norm = 1
      - W[1,0] → index 2 → mode_norm = 4
      - etc.

    This is the canonical SRT prescription for parameter tensors.
    See: theory/resonant_engine.md:138-140, winding_net_pure.py:111-115
    """
    
    def __init__(
        self,
        shape: List[int],
        init: str = 'uniform',
        init_scale: Optional[float] = None,
        requires_grad: bool = True,
        precision: int = 100,
    ):
        """
        Initialize parameter.
        
        Args:
            shape: Parameter shape
            init: Initialization method ('uniform', 'normal', 'kaiming', 'golden', 'zeros')
            init_scale: Scale for initialization (auto-computed if None)
            requires_grad: Whether parameter is trainable
            precision: ResonantTensor precision
        """
        self.shape = list(shape)
        self.requires_grad = requires_grad
        self.precision = precision
        
        # Compute size
        size = 1
        for d in shape:
            size *= d
        
        # Initialize data
        data = self._initialize(size, init, init_scale)
        mode_norms = [float(i * i) for i in range(size)]
        
        self.tensor = ResonantTensor(data, shape, mode_norms, precision)
    
    def _initialize(self, size: int, method: str, scale: Optional[float]) -> List[float]:
        """Initialize parameter data."""
        if method == 'zeros':
            return [0.0] * size
        
        if method == 'ones':
            return [1.0] * size
        
        # Auto-compute scale
        if scale is None:
            if len(self.shape) >= 2:
                fan_in = self.shape[-2] if len(self.shape) >= 2 else self.shape[-1]
                scale = 1.0 / math.sqrt(fan_in)
            else:
                scale = 0.1
        
        if method == 'uniform':
            return [random.uniform(-scale, scale) for _ in range(size)]
        
        if method == 'normal':
            return [random.gauss(0, scale) for _ in range(size)]
        
        if method == 'kaiming':
            std = math.sqrt(2.0 / self.shape[-1] if self.shape else 1.0)
            return [random.gauss(0, std) for _ in range(size)]
        
        if method == 'golden':
            # Use mode norm |n|² = i² (not index i) for variance scaling
            # Formula from SRT sub-Gaussian measure (resonant_embedding_pure.py:67)
            # Variance decays quadratically with mode norm: exp(-i²/(2φ))
            return [
                random.gauss(0, scale * math.exp(-(i * i) / (2 * PHI)))
                for i in range(size)
            ]
        
        return [random.gauss(0, scale) for _ in range(size)]
    
    def to_list(self) -> List[float]:
        """Get parameter data as list."""
        return self.tensor.to_floats()
    
    def __repr__(self) -> str:
        return f"Parameter(shape={self.shape}, requires_grad={self.requires_grad})"


class Module:
    """
    Base class for all syntonic network modules.
    
    Mirrors torch.nn.Module API for familiarity, but uses
    ResonantTensor for computations.
    """
    
    def __init__(self):
        self._parameters: Dict[str, Parameter] = {}
        self._modules: Dict[str, 'Module'] = {}
        self._buffers: Dict[str, Any] = {}
        self.training = True
    
    def __setattr__(self, name: str, value: Any):
        if isinstance(value, Parameter):
            self._parameters[name] = value
        elif isinstance(value, Module):
            self._modules[name] = value
        object.__setattr__(self, name, value)
    
    def register_buffer(self, name: str, value: Any):
        """Register a buffer (non-trainable state)."""
        self._buffers[name] = value
        object.__setattr__(self, name, value)
    
    def forward(self, *args, **kwargs):
        """Forward pass - override in subclasses."""
        raise NotImplementedError("Subclasses must implement forward()")
    
    def __call__(self, *args, **kwargs):
        """Make module callable."""
        return self.forward(*args, **kwargs)
    
    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:
        """Iterate over module parameters."""
        for param in self._parameters.values():
            yield param
        
        if recurse:
            for module in self._modules.values():
                for param in module.parameters(recurse=True):
                    yield param
    
    def named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[tuple]:
        """Iterate over named parameters."""
        for name, param in self._parameters.items():
            yield f"{prefix}{name}", param
        
        if recurse:
            for name, module in self._modules.items():
                subprefix = f"{prefix}{name}."
                for n, p in module.named_parameters(prefix=subprefix, recurse=True):
                    yield n, p
    
    def modules(self, recurse: bool = True) -> Iterator['Module']:
        """Iterate over modules."""
        yield self
        
        if recurse:
            for module in self._modules.values():
                for m in module.modules(recurse=True):
                    yield m
    
    def train(self, mode: bool = True) -> 'Module':
        """Set training mode."""
        self.training = mode
        for module in self._modules.values():
            module.train(mode)
        return self
    
    def eval(self) -> 'Module':
        """Set evaluation mode."""
        return self.train(False)
    
    def extra_repr(self) -> str:
        """Extra info for repr - override in subclasses."""
        return ''
    
    def __repr__(self) -> str:
        lines = [f"{self.__class__.__name__}("]
        extra = self.extra_repr()
        if extra:
            lines.append(f"  {extra}")
        for name, module in self._modules.items():
            mod_str = repr(module).replace('\n', '\n  ')
            lines.append(f"  ({name}): {mod_str}")
        lines.append(")")
        return '\n'.join(lines)


class Sequential(Module):
    """Sequential container for modules."""
    
    def __init__(self, *modules: Module):
        super().__init__()
        for i, module in enumerate(modules):
            self._modules[str(i)] = module
            setattr(self, str(i), module)
    
    def forward(self, x):
        for module in self._modules.values():
            x = module(x)
        return x
    
    def __len__(self) -> int:
        return len(self._modules)
    
    def __getitem__(self, idx: int) -> Module:
        return self._modules[str(idx)]


class ModuleList(Module):
    """Container for a list of modules."""
    
    def __init__(self, modules: Optional[List[Module]] = None):
        super().__init__()
        if modules is not None:
            for i, module in enumerate(modules):
                self._modules[str(i)] = module
                setattr(self, str(i), module)
    
    def append(self, module: Module):
        """Append a module."""
        idx = len(self._modules)
        self._modules[str(idx)] = module
        setattr(self, str(idx), module)
    
    def forward(self, x):
        raise NotImplementedError("ModuleList has no forward method")
    
    def __len__(self) -> int:
        return len(self._modules)
    
    def __getitem__(self, idx: int) -> Module:
        return self._modules[str(idx)]
    
    def __iter__(self) -> Iterator[Module]:
        return iter(self._modules.values())


class Dropout(Module):
    """Dropout layer."""
    
    def __init__(self, p: float = 0.5):
        super().__init__()
        self.p = p
    
    def forward(self, x: ResonantTensor) -> ResonantTensor:
        if not self.training or self.p == 0:
            return x
        
        data = x.to_floats()
        mask = [1.0 if random.random() > self.p else 0.0 for _ in data]
        scale = 1.0 / (1.0 - self.p)
        dropped = [d * m * scale for d, m in zip(data, mask)]
        return ResonantTensor(dropped, list(x.shape), [float(i*i) for i in range(len(dropped))], 100)
    
    def extra_repr(self) -> str:
        return f"p={self.p}"


class Identity(Module):
    """Identity layer - returns input unchanged."""
    
    def forward(self, x):
        return x


class ReLU(Module):
    """ReLU activation."""
    
    def forward(self, x: ResonantTensor) -> ResonantTensor:
        return x.relu()


class Sigmoid(Module):
    """Sigmoid activation."""
    
    def __init__(self, precision: int = 100):
        super().__init__()
        self.precision = precision
    
    def forward(self, x: ResonantTensor) -> ResonantTensor:
        return x.sigmoid(self.precision)


class Tanh(Module):
    """Tanh activation."""
    
    def __init__(self, precision: int = 100):
        super().__init__()
        self.precision = precision
    
    def forward(self, x: ResonantTensor) -> ResonantTensor:
        return x.tanh(self.precision)


def compute_spatial_mode_norms(shape: List[int]) -> List[float]:
    """
    Compute spatial mode norms for data tensors (NOT for parameters).

    Use this ONLY for tensors representing spatial data on a torus where
    multi-dimensional frequency structure matters (T⁴ states, field configs).

    For neural network parameters, use default 1D mode norms (automatic).

    Args:
        shape: Tensor shape [d0, d1, d2, ...]

    Returns:
        Mode norms where element at [i0, i1, ...] has:
        |n|² = (i0 - d0//2)² + (i1 - d1//2)² + ...

    Example:
        >>> # For T⁴ winding state representation
        >>> mode_norms = compute_spatial_mode_norms([8, 8, 8, 8])
    """
    import itertools

    mode_norms = []
    ranges = [range(d) for d in shape]

    for indices in itertools.product(*ranges):
        norm_sq = sum((idx - dim // 2) ** 2
                     for idx, dim in zip(indices, shape))
        mode_norms.append(float(norm_sq))

    return mode_norms


__all__ = [
    'Parameter',
    'Module',
    'Sequential',
    'ModuleList',
    'Dropout',
    'Identity',
    'ReLU',
    'Sigmoid',
    'Tanh',
    'PHI',
    'compute_spatial_mode_norms',
]
